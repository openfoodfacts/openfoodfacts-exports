# Data Exports Documentation

This document provides detailed information about the various data exports provided by Open Food Facts Exports service.

## Mobile App Export

### Overview
The mobile app export (`openfoodfacts-mobile-dump-products.tsv.gz`) is a highly optimized dataset specifically designed for mobile applications. It contains only essential product information to minimize download size and parsing time.

### Access
- **URL**: https://openfoodfacts-ds.s3.eu-west-3.amazonaws.com/openfoodfacts-mobile-dump-products.tsv.gz
- **Update Frequency**: Daily
- **Format**: Tab-separated values (TSV), gzipped
- **Size**: ~10x smaller than the complete dataset

### Fields Included
| Field | Description | Example |
|-------|-------------|---------|
| `code` | Product barcode/ID | `3017620422003` |
| `product_name` | Product name | `Nutella` |
| `quantity` | Package quantity | `400 g` |
| `brands` | Brand names | `Ferrero` |
| `nutrition_grade_fr` | Nutri-Score grade | `E` |
| `nova_group` | NOVA food processing level | `4` |
| `ecoscore_grade` | Eco-Score environmental grade | `D` |

### Generation Process
The mobile export is generated by the `generate_mobile_app_dump()` function in `openfoodfacts_exports/exports/csv.py`:

1. **Source**: Generated from the daily Parquet export
2. **Processing**: Uses DuckDB to extract and transform essential fields
3. **Output**: Tab-separated values with gzip compression
4. **Upload**: Automatically pushed to AWS S3 when `ENABLE_S3_PUSH=1`

### Usage Examples

#### Download and Parse (Python)
```python
import pandas as pd
import gzip
from io import BytesIO
import requests

# Download the mobile export
url = "https://openfoodfacts-ds.s3.eu-west-3.amazonaws.com/openfoodfacts-mobile-dump-products.tsv.gz"
response = requests.get(url)

# Load into pandas
df = pd.read_csv(BytesIO(response.content), compression='gzip', sep='\t')
print(f"Loaded {len(df):,} products")
```

#### Mobile App Integration (JavaScript)
```javascript
// Fetch and decompress
async function loadMobileData() {
  const response = await fetch(
    'https://openfoodfacts-ds.s3.eu-west-3.amazonaws.com/openfoodfacts-mobile-dump-products.tsv.gz'
  );
  
  const buffer = await response.arrayBuffer();
  const decompressed = pako.inflate(buffer, { to: 'string' });
  
  // Parse TSV
  const lines = decompressed.split('\n');
  const headers = lines[0].split('\t');
  const products = lines.slice(1).map(line => {
    const values = line.split('\t');
    return headers.reduce((obj, header, i) => {
      obj[header] = values[i];
      return obj;
    }, {});
  });
  
  return products;
}
```

### Implementation Details
The mobile export is triggered as part of the daily export workflow for Open Food Facts products:

```python
# In openfoodfacts_exports/tasks.py
if flavor is Flavor.off:
    high_queue.enqueue(
        generate_push_mobile_app_dump,
        PARQUET_DATASET_PATH[flavor],
        depends_on=export_parquet_job,
        job_timeout="3h",
    )
```

## Recent Changes Export

### Overview
The recent changes export (`openfoodfacts_recent_changes.jsonl.gz`) provides incremental updates to the Open Food Facts database, allowing applications to sync changes without downloading the entire dataset.

### Access
- **URL**: https://world.openfoodfacts.org/data/openfoodfacts_recent_changes.jsonl.gz
- **Update Frequency**: Daily
- **Format**: Newline-delimited JSON (JSONL), gzipped
- **Content**: Products that were created or modified in the last 24 hours

### Data Structure
Each line in the JSONL file represents a single product change:

```json
{
  "code": "3017620422003",
  "action": "updated",
  "timestamp": 1640995200,
  "product": {
    // Complete product data for changed/new products
    "product_name": "Nutella",
    "brands": "Ferrero",
    // ... all other product fields
  }
}
```

### Fields Description
| Field | Type | Description |
|-------|------|-------------|
| `code` | string | Product barcode/identifier |
| `action` | string | Type of change: `"created"`, `"updated"`, `"deleted"` |
| `timestamp` | integer | Unix timestamp of the change |
| `product` | object | Complete product data (null for deletions) |

### Usage Examples

#### Python Processing
```python
import gzip
import json
from datetime import datetime

# Process recent changes
with gzip.open('openfoodfacts_recent_changes.jsonl.gz', 'rt') as f:
    for line in f:
        change = json.loads(line.strip())
        
        timestamp = datetime.fromtimestamp(change['timestamp'])
        print(f"Product {change['code']} was {change['action']} at {timestamp}")
        
        if change['product']:
            product = change['product']
            print(f"  Name: {product.get('product_name', 'N/A')}")
```

#### Incremental Sync Pattern
```python
import requests
import json
import gzip
from datetime import datetime, timedelta

def sync_recent_changes(last_sync_timestamp=None):
    """Sync only products changed since last sync"""
    
    # Download recent changes
    url = "https://world.openfoodfacts.org/data/openfoodfacts_recent_changes.jsonl.gz"
    response = requests.get(url)
    
    changes_to_process = []
    
    with gzip.open(io.BytesIO(response.content), 'rt') as f:
        for line in f:
            change = json.loads(line.strip())
            
            # Only process changes after our last sync
            if last_sync_timestamp and change['timestamp'] <= last_sync_timestamp:
                continue
                
            changes_to_process.append(change)
    
    print(f"Processing {len(changes_to_process)} changes since last sync")
    
    for change in changes_to_process:
        if change['action'] == 'deleted':
            # Handle product deletion
            delete_local_product(change['code'])
        else:
            # Handle creation or update
            update_local_product(change['code'], change['product'])
    
    return datetime.now().timestamp()
```

### Integration with Main Dataset
The recent changes export complements the main datasets:

1. **Initial Sync**: Download complete Parquet/CSV dataset
2. **Daily Updates**: Download recent changes for incremental updates
3. **Periodic Full Sync**: Occasionally re-download complete dataset

### Note on Availability
⚠️ **Status**: The recent changes export is currently **undocumented in the main Open Food Facts data page** but is available for use. This export is likely generated by the main Open Food Facts server rather than this exports service.

## Data Format Comparison

| Format | Size | Use Case | Update Method | Best For |
|--------|------|----------|---------------|----------|
| **Parquet** | ~2GB | Data science, analytics | Full daily | Research, ML |
| **Mobile TSV** | ~200MB | Mobile apps | Full daily | Mobile apps |
| **Recent Changes** | ~50MB | Incremental sync | Daily changes | Real-time apps |
| **Full CSV** | ~4GB | General use | Full daily | Traditional tools |

## Rate Limits and Best Practices

### Download Guidelines
- **Respect bandwidth**: Don't download more frequently than daily updates
- **Use incremental sync**: Prefer recent changes over full re-downloads
- **Cache locally**: Store and reuse data to minimize requests
- **Compress storage**: Keep gzipped versions when possible

### Error Handling
- Implement retry logic with exponential backoff
- Validate file integrity after download
- Have fallback to cached data if download fails
- Monitor file sizes for unexpected changes

### Attribution Requirements
When using any Open Food Facts data:
- Credit: "Data from Open Food Facts"
- License: Open Database License (ODbL)
- Link back: https://openfoodfacts.org
- Share alike: Improvements must be shared back to the community